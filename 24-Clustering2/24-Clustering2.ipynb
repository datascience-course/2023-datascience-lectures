{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Data Science \n",
    "# Lecture 24: Clustering 2\n",
    "*COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/*\n",
    "\n",
    "In this lecture, we'll continue our discussion of clustering, covering\n",
    "* distances\n",
    "* hierarchical clustering \n",
    "* dendogram plots and heatmaps\n",
    "* DBSCAN\n",
    "* A comparison of clustering methods on MNIST digits\n",
    "\n",
    "Recommended Reading:\n",
    "* G. James, D. Witten, T. Hastie, and R. Tibshirani, An Introduction to Statistical Learning, Ch. 10.1 and 10.3. [digitial version available here](http://www-bcf.usc.edu/~gareth/ISL/)\n",
    "* J. Grus, Data Science from Scratch, Ch. 19\n",
    "* [scikit-learn documentation on clustering](http://scikit-learn.org/stable/modules/clustering.html)\n",
    "* [Jörn's SciPy Hierarchical Clustering and Dendrogram Tutorial](https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Partitional Clustering with K-Means\n",
    "\n",
    "[Clustering](https://en.wikipedia.org/wiki/Cluster_analysis) is the task of discovering unknown subgroups in data, which we call *clusters*.  In other words, the **goal** is to partition the datset into clusters where ‘similar’ items are in the same cluster and ‘dissimilar’ items are in different clusters. \n",
    "\n",
    "**Examples:**\n",
    "* Social Network Analysis: Clustering can be used to find communities.\n",
    "* Ecology: cluster organisms that share attributes into species, genus, etc...\n",
    "* Genetics: cluster tissue samples by similar gene expression. \n",
    "* Handwritten digits where the digits are unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The k-means clustering method\n",
    "\n",
    "**Data:**  A collection of points $\\{x_i\\}$, for $i = 1,\\ldots n$, where $x_i\\in \\mathbb R^d$. \n",
    "\n",
    "In [k-means clustering](https://en.wikipedia.org/wiki/K-means_clustering), one tries to find $k$ *centers*, $\\{\\mu_\\ell\\}$, $\\ell = 1,\\ldots k$, and assign each point $x$ to a *cluster* $C_\\ell$ with center $\\mu_\\ell$, as to minimize the *total intra-cluster distance* \n",
    "$$\n",
    "\\arg\\min_{C} \\sum_{\\ell=1}^k \\sum_{x_i \\in C_\\ell} \\| x_i - \\mu_\\ell\\|^2. \n",
    "$$\n",
    "Here, $\\mu_\\ell$ is the mean of points in $C_\\ell$. The total intra-cluster distance is the total squared Euclidean distance from each point to the center of its cluster. It's a measure of the variance or internal coherence of the clusters. \n",
    "\n",
    "We can take a look at an interactive illustration:\n",
    "https://www.naftaliharris.com/blog/visualizing-k-means-clustering/\n",
    "### Lloyd's Algorithm\n",
    "\n",
    "\n",
    "**Input:** set of points $x_1,\\ldots, x_n$ and an integer $k$ (# clusters)\n",
    "\n",
    "Pick $k$ starting points as centers $\\mu_1, \\ldots, \\mu_k$.\n",
    "\n",
    "**while** not converged:\n",
    "1. Assign each point $x_i$, to the cluster $C_\\ell$ with closest center $\\mu_\\ell$. \n",
    "2. For each cluster $C_\\ell$, compute a new center, $\\mu_\\ell$, by taking the mean of all $x_i$ assigned to cluster $C_\\ell$, *i.e.*, \n",
    "$$\n",
    "\\mu_\\ell = \\frac{1}{|C_\\ell|}\\sum_{x_i \\in C_\\ell} x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lloyd's Algorithm Illustrated\n",
    "\n",
    "\n",
    "![Lloyd's Algorithm Illustrated](lloyd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Performance and properties of k-means\n",
    "\n",
    "* The run time is $O(n*k*d*i)$ where \n",
    " - n is the number of items,\n",
    " - k is the number of clusters\n",
    " - d is the number of dimensions of the feature vectors\n",
    " - i is the number of iterations needed until convergence. \n",
    " \n",
    "  For data that has well-defined clusters, $i$ is typically small. In practice, the $k$-means algorithm is very fast. \n",
    "\n",
    "* Lloyds algorithm finds a *local optimum*, not necessarily the *global optimum*\n",
    "\n",
    "  Since the algorithm is fast, it is common to run the algorithm multiple times and pick the solution with the smallest total intra-cluster distance, \n",
    "$$\n",
    "\\sum_{\\ell=1}^k \\sum_{x_i \\in C_\\ell} \\| x_i - \\mu_\\ell\\|^2. \n",
    "$$\n",
    "\n",
    "* The total intra-cluster distance doesn't increase at every iteration of Lloyd's algorithm\n",
    "\n",
    "* The total intra-cluster distance decreases with larger $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Cluster Evaluation \n",
    "\n",
    "Recall that we discussed different methods for evaluating clusters, either with or without labels. \n",
    "\n",
    "#### Without ground-truth labels\n",
    "\n",
    "+ Visual comparison\n",
    "+ Use the total intra-cluster distance (useful for k-means)\n",
    "+ [Silhouette analysis](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)\n",
    "\n",
    "\n",
    "#### With ground-truth labels\n",
    "\n",
    "+ [`homogeneity_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html): Homogeneity metric of a cluster labeling given a ground truth. A clustering result satisfies homogeneity if all of its clusters contain only data points which are members of a single class.\n",
    "+ [`completeness_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.completeness_score.html): A clustering result satisfies completeness if all the data points that are members of a given class are elements of the same cluster.\n",
    "+ [`v_measure_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.v_measure_score.html): The V-measure is the harmonic mean between homogeneity and completeness:\n",
    "$$\n",
    "v = 2 * (homogeneity * completeness) / (homogeneity + completeness)\n",
    "$$\n",
    "+ [`homogeneity_completeness_v_measure`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_completeness_v_measure.html): Compute the homogeneity, completeness, and v-Measure scores at once.\n",
    "+ Confusion matrix \n",
    "\n",
    "*Note:* Homogeneity score does NOT account for over-clustering and completeness score does NOT account for under-clustering. Thus it is important to combine these metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Measures of distance\n",
    "\n",
    "All clustering methods operate on a measure of distance. There are various distance measures that are useful depending on the context.\n",
    "\n",
    "+ **Euclidean distance**:\n",
    "$$\n",
    "d(x,y) = \\sqrt{\\sum_{i=1}^d (x_i - y_i)^2 }\n",
    "$$\n",
    "+ **Manhattan distance**:\n",
    "$$\n",
    "d(x,y) = \\sum_{i=1}^d |x_i - y_i|\n",
    "$$\n",
    "Measures distance as you would travel in a grid, as in city blocks in Manhattan. \n",
    "+ **Correlation**: \n",
    "$$\\frac{\\sum ^n _{i=1}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum ^n _{i=1}(x_i - \\bar{x})^2} \\sqrt{\\sum ^n _{i=1}(y_i - \\bar{y})^2}}\n",
    "$$\n",
    "where $\\bar{x}=\\frac{1}{n}\\sum_{i=1}^n x_i$ and $\\bar{y}=\\frac{1}{n}\\sum_{i=1}^n y_i$ are the means. \n",
    "Note: correlation is a measure of *similarity* which is inversely related to distance.\n",
    "\n",
    "+ **Jaccard Distance**: If $A$ and $B$ are two sets, we define the Jaccard similarity coefficient\n",
    "$$\n",
    "J(A,B) = \\frac{ |A\\cap B|}{ |A\\cup B| }.\n",
    "$$\n",
    "We always have that $0 \\leq J(A,B) \\leq 1$. We then define the **Jaccard  distance** as \n",
    "$$\n",
    "d(A,B) = 1 - J(A,B).\n",
    "$$\n",
    "\n",
    "### Which distance should you use when? \n",
    "\n",
    "* Euclidean distance works well for roughly normally distributed data, where absolute differences matter. \n",
    "* Manahttan distance works better with datasets with many outliers (that would be improbable in normally distributed data).\n",
    "* Correlation measures work well if the absolute values aren't as important, but correlation is. An example use case is gene-expression data that is unitless and can only be compared in relative termes. \n",
    "* Jaccard Distance is useful for binary/categorical data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "[Hierarchical Clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering) is a collection of methods for clustering, where we don't just find a single clustering of the data, but a hierarchy of clusters. There are two main strategies for hierarchical clustering:\n",
    "\n",
    "+ **Agglomerative:** This \"bottom up\" approach to clustering starts with each data point in its own cluster. Pairs of clusters are merged as one moves up the hierarchy.\n",
    "\n",
    "+ **Divisive:** This \"top down\" approach to clustering starts with all observations in one cluster. Splits of the clusters are made recursively as one moves down the hierarchy.\n",
    "\n",
    "We'll focus on Agglomerative Clustering.\n",
    "\n",
    "\n",
    "### Agglomerative clustering\n",
    "* Start with each item as it’s own cluster.\n",
    "+ Link together the two clusters that are 'closest together' and store this information in the dendrogram plot. \n",
    "+ Continue this process until there is only one cluster.\n",
    "+ Using the dendrogram plot, decide which clustering is best.\n",
    "\n",
    "<img src=\"dendrogram.png\" width=\"500\">\n",
    "\n",
    "### Linkage methods in Agglomerative clustering:\n",
    "* **Maximum or complete linkage**: the maximum distance between observations of pairs of clusters, \n",
    "$$\n",
    "\\max\\{d(a,b)\\colon a \\in A, b \\in B \\}.\n",
    "$$\n",
    "\n",
    "* **Minimum or single linkage**: the minimum distance between observations of pairs of clusters, \n",
    "$$\n",
    "\\min\\{d(a,b)\\colon a \\in A, b \\in B \\}.\n",
    "$$\n",
    "\n",
    "* **Average linkage**: the average of the distances between all observations of pairs of clusters,\n",
    "$$\n",
    "\\frac{1}{|A| |B|} \\sum_{a \\in A, b \\in B} d(a, b).\n",
    "$$\n",
    "\n",
    "* **Centroid distance**:  if $c_A$ and $c_B$ are the centers of clusters $A$ and $B$, then $d(c_A,c_B)$.\n",
    "\n",
    "* **Ward** minimizes the total within-cluster distance, similiar to $k$-means.  \n",
    "\n",
    "![Comparison of Clustering with Different Linkages](ComparisonOfClusteringMethods.png)\n",
    "\n",
    "Image [source](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agglomerative Clustering in SciKit Learn\n",
    "\n",
    "We can use SciKit Learn's [agglomerative clustering implementation](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.cluster import hierarchy \n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_blobs, make_moons, load_iris, load_digits\n",
    "from sklearn.cluster import *\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics import homogeneity_score, homogeneity_completeness_v_measure\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "# Create color maps\n",
    "#cmap = ListedColormap([\"Red\",\"Green\",\"Blue\"])\n",
    "color_array = [\"#984ea3\",\"#a65628\",\"#ffff33\",\"#4daf4a\",\"#ff7f00\", \"#e41a1c\", \"#377eb8\"]\n",
    "cmap = ListedColormap(color_array)\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we pass a number of clusters, because we can't see the hierarchy in this plot. Try changing the number of clusters to understand the behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=3, random_state=1)\n",
    "\n",
    "# if we know there are 3 clusters\n",
    "agg_cluster_model = AgglomerativeClustering(linkage='average', metric='euclidean', n_clusters=3)\n",
    "y_pred = agg_cluster_model.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred,  marker=\"o\", cmap=cmap); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a dendogram plot\n",
    "\n",
    "A [dendogram plot](https://en.wikipedia.org/wiki/Dendrogram) can be used to decide on the number of clusters.\n",
    "\n",
    "The `scipy.cluster.hierarchy` function [`linkage`](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage) returns an array of length $n-1$ that contains all cluster merging information. Each row has the format \n",
    "`[idx1, idx2, dist, sample_count]`.\n",
    "\n",
    "The `scipy.cluster.hierarchy` function [`fcluster`](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.fcluster.html) can then be used to extract the clusters from the linkage array and [`dendrogram`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html#scipy.cluster.hierarchy.dendrogram) to display the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = hierarchy.linkage(X, 'ward') # generate the linkage array\n",
    "print(Z[:5])\n",
    "#print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dendrogram plot\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "hierarchy.set_link_color_palette(color_array)\n",
    "hierarchy.dendrogram(Z,\n",
    "    leaf_rotation=90.,  # rotates the x axis labels\n",
    "    leaf_font_size=8. # font size for the x axis labels\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = hierarchy.fcluster(Z=Z, t=3, criterion='maxclust')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred,  marker=\"o\", cmap=cmap); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: two moons dataset\n",
    "\n",
    "By default, agglomorative clustering doesn't take any connectivity into account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_moons, y = make_moons(n_samples=500, noise=.05)\n",
    "\n",
    "agg_cluster_model = AgglomerativeClustering(linkage=\"complete\", metric='euclidean', n_clusters=2)\n",
    "y_pred = agg_cluster_model.fit_predict(X_moons)\n",
    "\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_pred,  marker=\"o\", cmap=cmap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Adding connectivity constraints\n",
    "\n",
    "Previously, we joined clusters based soley on distance. Here we introduce a [connectivity constraint](http://scikit-learn.org/stable/auto_examples/cluster/plot_ward_structured_vs_unstructured.html#sphx-glr-auto-examples-cluster-plot-ward-structured-vs-unstructured-py) based on k-Nearest Neighbors graph so that only adjacent clusters can be merged together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "connectivity = kneighbors_graph(X_moons, n_neighbors=10, include_self=False)\n",
    "\n",
    "\n",
    "agg_cluster_model = AgglomerativeClustering(linkage=\"complete\", connectivity=connectivity, n_clusters=2,compute_full_tree=True)\n",
    "y_pred = agg_cluster_model.fit_predict(X_moons)\n",
    "\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_pred,  marker=\"o\", cmap=cmap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of connectivity constraints on various linkages\n",
    "\n",
    "![Clustering without connectivity constraints](connectivity_plot1.png)\n",
    "![Clustering with connectivity constraints](connectivity_plot2.png)\n",
    "\n",
    "Image [source](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_clustering.html#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-py) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: IRIS Dataset\n",
    "\n",
    "Recall the Iris dataset consists of 4 measurements for 150 different examples of irises. We know that there are $k=3$  species of irises in the dataset. Without using the labels, let's try to find them. Note: this is a harder problem than classification since we're not using the (known) labels! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# load dataset \n",
    "iris = load_iris()\n",
    "\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "agg_cluster_model = AgglomerativeClustering(linkage=\"ward\", metric='euclidean', n_clusters=3)\n",
    "y_pred = agg_cluster_model.fit_predict(X_iris)\n",
    "\n",
    "plt.scatter(X_iris[:, 0], X_iris[:, 1], c=y_pred,  marker=\"o\", cmap=cmap);\n",
    "metrics.v_measure_score(labels_true=y_iris, labels_pred=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the plot with the ground truth labels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_iris[:, 0], X_iris[:, 1], c=y_iris,  marker=\"o\", cmap=cmap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we're plotting only two dimensions of the dataset here, so it's tricky to see whether the clustering works well. \n",
    "\n",
    "A useful method to visuliaze a high-dimensional dataset are **cluster heatmaps**. Cluster heatmaps combine the dendrogram with a heat map, so you can judge the values of each dimension in a cluster. \n",
    "\n",
    "[Seaborn](https://seaborn.pydata.org/generated/seaborn.clustermap.html) provides a nice ready made cluster heatmap. This calles the scikit learn clustering algorithms behind the scenes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "species = iris.pop(\"species\")\n",
    "\n",
    "# the color for the labels\n",
    "lut = dict(zip(species.unique(), color_array))\n",
    "print(lut)\n",
    "row_colors = species.map(lut)\n",
    "\n",
    "# for method, try \"single\", \"average\", \"ward\"\n",
    "# for method try \"correlation\", \"euclidean\", \"cityblock\"\n",
    "g = sns.clustermap(iris, method=\"average\", metric=\"euclidean\", row_colors=row_colors, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we add interactivity, we can also dynamically \"cut\" the dendrogram. See [this video](https://youtu.be/8SV3Id_lvNY?t=129), for example.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Evaluating the clusters\n",
    "\n",
    "We can use the homogeneity/completeness/v-measure scores to evaluate our clusters. Here we'll try a couple of different configurations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "homogeneity_completeness_v_measure(labels_true = y_iris, labels_pred = y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look at these scores for a couple of different parameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "models = [AgglomerativeClustering(linkage=\"ward\", metric='euclidean', n_clusters=3), \n",
    "         AgglomerativeClustering(linkage=\"average\", metric='euclidean', n_clusters=3), \n",
    "         AgglomerativeClustering(linkage=\"complete\", metric='euclidean', n_clusters=3), \n",
    "         AgglomerativeClustering(linkage=\"average\", metric='manhattan', n_clusters=3),\n",
    "         AgglomerativeClustering(linkage=\"complete\", metric='manhattan', n_clusters=3),\n",
    "         KMeans(n_clusters=3, n_init=10)]\n",
    "h = np.zeros([len(models),3])\n",
    "for i,m in enumerate(models):\n",
    "    y_pred = m.fit_predict(X_iris)\n",
    "    h[i,:] = homogeneity_completeness_v_measure(labels_true = y_iris, labels_pred = y_pred)\n",
    "    print(h[i,:])\n",
    "\n",
    "print('The winner is model #' + str(np.argmax(h[:,2]) + 1)\n",
    "      + ' with V-Measure ' + str(np.max(h[:,2])))\n",
    "print('Method details:')\n",
    "print(models[np.argmax(h[:,0])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude that the average clustering method with the euclidean distance is the best match for this dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBScan\n",
    "\n",
    "Density-based spatial clustering of applications with noise (DBSCAN) is an algorithm that is based on the premise that clusters are dense clouds of points, and that points that are in the same cluster should be connected by a high density region. \n",
    "\n",
    "We need to choose two parameters: \n",
    " * $\\epsilon$, a measure for distance between two points, and \n",
    " * *minPoints*, a measure for the minimum number of points in a cluster.\n",
    "\n",
    "Here is how the algorithm works:\n",
    "\n",
    " 1. We start by picking a random point in our dataset. \n",
    " 2. We look how many points are within the distance $\\epsilon$ of that point. If there are more than *minPoints* we create a cluster and add all of those points. If not, we consider the point an outlier and move on to step 4. \n",
    " 3. We check all new points using step 2 and add those neighbors and continue recursively doing so until no more points can be added. \n",
    " 4. When we run out of points, we pick a new random point from the set of points that doesn't belong to a cluster. \n",
    "\n",
    "DBSCAN is notable for it's explicit treatment of outliers and for its ability to work with complex shapes. \n",
    " \n",
    "\n",
    "We can take a look at an interactive illustration:\n",
    "https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/ \n",
    " \n",
    " \n",
    "Scikit-learn also has a [DBSCAN implemetation](https://scikit-learn.org/stable/modules/clustering.html#dbscan). Note the outliers shown in the examples below. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "X, y = make_blobs(n_samples=200, centers=3, random_state=1)\n",
    "\n",
    "# try changeing eps and min_samples\n",
    "db_model = DBSCAN(eps=1.0, min_samples=2)\n",
    "db_model.fit(X)\n",
    "y_pred = db_model.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred,  marker=\"o\", cmap=cmap); \n",
    "\n",
    "labels = db_model.labels_\n",
    "\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(y, labels))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(y, labels))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(y, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN works well for uneven sized clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=1000, centers=3, random_state=170)\n",
    "\n",
    "# Unevenly sized blobs\n",
    "X_filtered = np.vstack((X[y == 0][:200], X[y == 1][:50], X[y == 2][:10]))\n",
    "\n",
    "\n",
    "# try changeing eps and min_samples\n",
    "db_model = DBSCAN(eps=1.5, min_samples=5)\n",
    "db_model.fit(X_filtered)\n",
    "y_pred = db_model.fit_predict(X_filtered)\n",
    "\n",
    "plt.scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_pred,  marker=\"o\", cmap=cmap); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also works well for non-convex shapes, no extra tricks required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_moons, y = make_moons(n_samples=500, noise=.05)\n",
    "\n",
    "# try changeing eps and min_samples\n",
    "db_model = DBSCAN(eps=0.2, min_samples=5)\n",
    "db_model.fit(X_moons)\n",
    "y_pred = db_model.fit_predict(X_moons)\n",
    "\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_pred,  marker=\"o\", cmap=cmap); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=1000, centers=3, random_state=3)\n",
    "\n",
    "# Anisotropically distributed data\n",
    "transformation = [[ 0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "\n",
    "# try changeing eps and min_samples\n",
    "db_model = DBSCAN(eps=0.30, min_samples=5)\n",
    "db_model.fit(X_aniso)\n",
    "y_pred = db_model.fit_predict(X_aniso)\n",
    "\n",
    "plt.scatter(X_aniso[:, 0], X_aniso[:, 1], c=y_pred,  marker=\"o\", cmap=cmap); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Iris dataset, it's hard for DBSCAN to tease apart the two correlated species. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_cluster_model = DBSCAN(eps=0.55, min_samples=8)\n",
    "y_pred = dbscan_cluster_model.fit_predict(X_iris)\n",
    "\n",
    "plt.scatter(X_iris[:, 0], X_iris[:, 1], c=y_pred,  marker=\"o\", cmap=cmap);\n",
    "metrics.v_measure_score(labels_true=y_iris, labels_pred=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBScan doesn't work with clusters of unequal density. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=4, cluster_std=[0.7, 1, 3, 0.4], random_state=112)\n",
    "\n",
    "# Unevenly spread blobs\n",
    "y_pred = DBSCAN(eps=2, min_samples=3).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred,  marker=\"o\", cmap=cmap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros of DBSCAN: \n",
    "\n",
    "* Works for non-gaussian shapes. \n",
    "* Explicitly deals with outliers.\n",
    "* We don't have to specify the nubmer of clusters. \n",
    "\n",
    "Cons: \n",
    "\n",
    "* Sensitve to parameters.\n",
    "* Cannot cluster datasets with differences in densities between clusters – we have only one epsilon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Class Exercise: MNIST dataset"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

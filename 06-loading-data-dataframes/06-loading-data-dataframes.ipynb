{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "50a40f10-f4b6-4dd3-b7aa-c63ed3ca3244"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Data Science – Lecture 6: Loading Data, Dataframes\n",
    "*COMP 5360/ MATH 4100, University of Utah, http://datasciencecourse.net/*\n",
    "\n",
    "In this lecture, we will learn how to read in and write files and then finally cover pandas dataframes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Office Hours Update\n",
    "\n",
    "Tuesday Office Hours are now at 6:30 PM over Zoom "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Series Recap\n",
    "\n",
    "In the previous lab we've introduced pandas series, a one-dimensional data structure. You will see that dataframes behave very similar to series, so we will only quickly recap the most important aspects.\n",
    "\n",
    "Let's start by importing pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series have an index, and associated data. We can use the index to access the data. We've also learned that we can directly access indices and values through their implicit location. If we add the implicit location, a series looks like this: \n",
    "\n",
    "\n",
    "| Location (Implicit)| Index | Value | \n",
    "| - | - | - |\n",
    "|0| Stones     |    1962\n",
    "|1| Beatles    |    1960\n",
    "|2| Zeppelin     |  1968\n",
    "|3| Pink Floyd |    1965\n",
    "|4| Pink Floyd |    2012\n",
    "\n",
    "Here is the example in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_founded = pd.Series([1962, 1960, 1968, 1965, 2012],\n",
    "                         name=\"founded\",\n",
    "                         index=[\"Stones\", \"Beatles\", \"Zeppelin\", \"Pink Floyd\", \"Pink Floyd\"])\n",
    "bands_founded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when we access data with multiple indices, we don't get a simple data type, as in the above cases, but instead get another series back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_founded[\"Pink Floyd\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap: Indexing and slicing\n",
    "\n",
    "Indexing and slicing works largely like in normal python, but instead of just directly using the bracket notations, it is recommended to use `iloc` for indexing by position and `loc` for indexing by labelled indices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing by position\n",
    "bands_founded.iloc[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When slicing by labelled index, the last value specified is *included*, which differs from regular Python slicing behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing by index\n",
    "bands_founded.loc[\"Zeppelin\" : \"Pink Floyd\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both, `iloc` and `loc` can be used with arrays, which isn't possible in vanilla Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bands_founded.iloc[[0,3]])\n",
    "print(bands_founded.loc[[\"Beatles\", \"Pink Floyd\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, all these variants can also be used with boolean arrays, which we will soon find out to be very helpful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_founded.loc[[True, False, False, True, False]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap: Masking and Filtering\n",
    "\n",
    "With pandas we can create boolean arrays that we can use to mask and filter a dataset. In the following expression, we'll create a new array that has \"True\" for every band formed after 1964:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = bands_founded > 1964\n",
    "mask\n",
    "bands_founded[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: Exploring a Series\n",
    "\n",
    "There are various way we can explore a series. We can count the number of non-null values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = pd.Series([1962, 1960, 1968, 1965, 2012, None, 2016])\n",
    "numbers.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = numbers.dropna()\n",
    "numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works also for non-numerical data. Of course, we get different measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_founded.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Recap: Sorting \n",
    "\n",
    "We can sort a series by value or index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers.sort_values() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And make the sorting descending: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_numbers = numbers.sort_index(ascending=False)\n",
    "sorted_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the indices remain the same! We can **reset the indices**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we don't specify drop to be true, the previous indices are preserved in a separte column\n",
    "new_sorted_numbers = sorted_numbers.reset_index(drop=True)\n",
    "new_sorted_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying a Function\n",
    "\n",
    "Often, we will want to apply a function to all values of a Series. We can do that with the [`map()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Convert an integer year into a date, assuming Jan 1 as day and month.\n",
    "def to_date(year):\n",
    "    return datetime.date(int(year), 1, 1)\n",
    "    \n",
    "new_sorted_numbers.map(to_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an incredibly powerful concept that you can use to modify series in sophisticated ways, similar to list comprehension. \n",
    "\n",
    "Another way to use the map function is to pass in a dictionary that is then applied to matching objects: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sorted_numbers.map({1965:1945, 2012:1999, 1968:\"What\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Reading Data\n",
    "\n",
    "Up to now, we've mainly used data that we've specified directly in code. This is, of course, not particularly scalable. We want to load data from files and eventually also connect to databases and APIs. \n",
    "\n",
    "Data is often stored in structured file formats, such as CSV, JSON, or XML. We'll encounter all of these file formats in this class.\n",
    "\n",
    "JSON\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"fruit\": \"Apple\",\n",
    "    \"size\": \"Large\",\n",
    "    \"color\": \"Red\",\n",
    "    \"batches\": [15, 17, 12],\n",
    "    \"producer\": {\n",
    "        name: Orchard\n",
    "        founded: 1972\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "XML\n",
    "```xml\n",
    "<note>\n",
    "<to>Students</to>\n",
    "<from>Prof</from>\n",
    "<heading>Reminder</heading>\n",
    "<body>HW2 due this Friday at 11:59pm!</body>\n",
    "</note>\n",
    "```\n",
    "\n",
    "The simplest form is a **CSV (Comma Separated Values)** file. CSV isn't a formal file format, rather it's a table represented as a text file where the cells are separated by a delimiter. Commonly, the first row represents the header. A delimiter can be a tab character, a semicolon, a colon, etc. \n",
    "\n",
    "Many CSV files also have a special convention for dealing with text that could include the delimiter. The following text would be very hard to parse otherwise:\n",
    "```\n",
    "Artist, Album, Genre\n",
    "Michael Jackson, Bad, Pop, Funk, Rock\n",
    "``` \n",
    "\n",
    "Here, the album is of multiple genres which are separated by a comma. The comma, however, is also used to delimit the individual columns. \n",
    "\n",
    "To work around that, double-quotes are commonly used (though other escape characters are possible) to indicate that all the elements contained within the quotes are not meant to be delimiters:\n",
    "\n",
    "```\n",
    "Artist, Album, Genre\n",
    "Michael Jackson, Bad, \"Pop, Funk, Rock\"\n",
    "``` \n",
    "\n",
    "Of course, that's problematic if your text contains double-quotes. In that case, you'd have to *escape* them with a special character, such as a `\\`. \n",
    "\n",
    "Now, it is clear that `Pop, Funk, Rock` should belong in a single cell. \n",
    "\n",
    "We've prepared a dataset based on Wikipedia's [list of best-selling albums](https://en.wikipedia.org/wiki/List_of_best-selling_albums) in the file [hit_albums.csv](./hit_albums.csv). \n",
    "\n",
    "Here is what the first couple of lines look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Artist,Album,Released,Genre,\"Certified sales (millions)\",Claimed sales (millions)\n",
    "Michael Jackson,Thriller,1982,\"Pop, rock, R&B\",45.4,65\n",
    "AC/DC,Back in Black,1980,Hard rock,25.9,50\n",
    "Pink Floyd,The Dark Side of the Moon,1973,Progressive rock,22.7,45\n",
    "Whitney Houston / Various artists,The Bodyguard,1992,\"Soundtrack/R&B, soul, pop\",27.4,44\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various ways of reading a CSV file. We'll first cover the basic read (and write) operations of Python, but will quickly move on to specific parsers for CSV files in Python and in Pandas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic File Operations\n",
    "\n",
    "To read a file we first have to open it by specifying the file path, and specifying whether we want to read (r), write (w), both (r+), or append (a). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albums_file = open('hit_albums.csv', 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read a whole file at once. Notice that lines are terminated with a special character, a linefeed or newline character specified as `\\n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = albums_file.read()\n",
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we print this instead, `\\n` is translated into a newline: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading a file, we have to manually close it again to release the operating system resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albums_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative to reading the whole file, we can read each line separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albums_file = open('hit_albums.csv', 'r')\n",
    "line1 = albums_file.readline();\n",
    "print(line1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now [`split()`](https://docs.python.org/3/library/stdtypes.html#str.split) the string based on the comma, to create a simple CSV parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line1.split(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have called  `readline()`, the next time you call it, it will read the next line. \n",
    "\n",
    "We can also treat `albums_file` as a list (or better, as an iterable data structure) and loop over the file and read the data into an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for line in albums_file:\n",
    "    data.append(line.split(\",\"))\n",
    "    \n",
    "# let's not forget to close the file:\n",
    "albums_file.close()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now read individual cells or rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this **didn't take proper care of our double-quote escape of \"Pop, rock, R&B\"**. Also, numbers are still treated as strings and the newline character is also appended to the last cell. \n",
    "\n",
    "We could certainly improve our parser to handle these issues, but fortunately, there are existing methods to parse CSV files that make this easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing\n",
    "\n",
    "We can write by opening a file using the `w` flag. Here we also use the [`with`](https://docs.python.org/3/reference/compound_stmts.html#the-with-statement) keyword, which takes care of closing the file for us, even if things go wrong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('my_file.txt', 'w') as new_file:\n",
    "    new_file.write(\"Hello World\\nAre you still spinning?\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check out this file by opening [my_file.txt](my_file.txt). Notice that the file is only guaranteed to be written if you actually close it (which, here, is take care of by the context manager invoked by the `with` statement). \n",
    "\n",
    "You can find more examples on basic file operations in the [Python Documentation](https://docs.python.org/3/tutorial/inputoutput.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a CSV file with the CSV Library\n",
    "\n",
    "We can use the CSV library to help with reading the data. It takes a `delimiter` and a `quotechar` as parameters; the latter is useful for our double quotes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the csv library\n",
    "import csv\n",
    "\n",
    "# initialize the top-level array\n",
    "data_values = []\n",
    "\n",
    "# open the file and append rows as arrays to the data_values\n",
    "with open('hit_albums.csv') as csvfile:\n",
    "    # note that we can interchangably use ' and \" in general\n",
    "    # for the quotechar, however we use ' so that we can use \" without escaping\n",
    "    filereader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    # the row here is an array\n",
    "    for row in filereader:\n",
    "        print(\"Row: \" + str(row))\n",
    "        data_values.append(row)\n",
    "\n",
    "# Store the header in a separate array\n",
    "header = data_values.pop(0)\n",
    "   \n",
    "print()    \n",
    "print(header)\n",
    "print()\n",
    "print(data_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do computation on the numerical dimensions of this table, we need to convert the strings to numbers. If we just do this the simple way, it won't work: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in data_values: \n",
    "    row[2] = int(row[2])\n",
    "    row[4] = float(row[4])\n",
    "    row[5] = float(row[5])\n",
    "        \n",
    "data_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because the last column, `Claimed sales (millions)` doesn't have values for each row. In that case, the conversion throws the above `ValueError`. \n",
    "\n",
    "These errors are also called Exceptions. [Exceptions](https://docs.python.org/3/reference/compound_stmts.html#try) are error states that can be raised and caught:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in data_values: \n",
    "    # need to try and catch the exception because the column contains NaN values\n",
    "    try:\n",
    "        row[2] = int(row[2])\n",
    "        row[4] = float(row[4])\n",
    "        row[5] = float(row[5])\n",
    "    except ValueError: \n",
    "        row[5] = None\n",
    "    \n",
    "data_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, here we have matrix that we could work with. In reality, we probably would want to structure the data a little differently: \n",
    "\n",
    "Instead of treating each row as an array, we'd want to treat each dimension (column) as an array, as this is consistent with the rule of keeping the values of an array of the same type and makes the column homogeneous and it makes it easy to calculate means, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading CSV with Pandas\n",
    "\n",
    "Now, let's take a look at what it takes to read this file using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hit_albums = pd.read_csv(\"hit_albums.csv\")\n",
    "hit_albums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that was different! \n",
    "\n",
    "Pandas provides the insanely powerful ['read_csv()'](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) method. Also see [the documentation](http://pandas.pydata.org/pandas-docs/stable/io.html) for more info on all I/O (input/output) operations in pandas, including writing CSV files. \n",
    "\n",
    "You can pass a lot of arguments to the method, such as delimiter, quote-chars, etc., but for our case the default parameters just worked. \n",
    "\n",
    "We've also just created our first data frame! Let's look at data frames in detail next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Frames\n",
    "\n",
    "A data frame is a column-oriented data structure where each column is a pandas series.\n",
    "\n",
    "Remember the [cheat sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf) linked from the last lecture.\n",
    "\n",
    "We've already loaded a data frame from file, but for completeness sake, let's create one in code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandInfo = pd.DataFrame({\n",
    "        \"Name\":[\"Led Zeppelin\", \"The Beatles\", \"Rolling Stones\", \"Radiohead\"],\n",
    "        \"No Members\":[4, 4, 4, 5],\n",
    "        \"No Albums\":[9, 12, 29 ,9]\n",
    "    })\n",
    "bandInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe was initialized with a dictonary of column headers as keys and column data as values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as a series, a data frame has an index, which corresponds to the first column here. In this case the index was automatically generated, but as for the series, we could use explicit values for the index. \n",
    "\n",
    "We can access columns in a data frame, which returns a series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandInfo[\"Name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bandInfo[\"Name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And obviously, we can do all the things we've learned about to this column/series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous example used columns to create the data frame. We can also create a data frame from rows. This doesn't make a ton of sense in this example, but data could be available like this from your data source, like if you're parsing a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandInfo2 = pd.DataFrame([\n",
    "        {\"Name\":\"Led Zeppelin\", \"No Albums\":9, \"No Members\":4},\n",
    "        {\"Name\":\"The Beatles\", \"No Albums\":12, \"No Members\":4},\n",
    "        {\"Name\":\"Rolling Stones\", \"No Albums\":29, \"No Members\":4},\n",
    "        {\"Name\":\"Radiohead\", \"No Albums\":9, \"No Members\":5},\n",
    "    ])\n",
    "bandInfo2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While a series has only one axis, a dataframe has two, one for the rows (the index or '0' axis), one for the columns (the column or '1' axis). We can check out these axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandInfo.axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access these axes directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The row axis\n",
    "bandInfo.axes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The columns axis\n",
    "bandInfo.axes[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Data Frames\n",
    "\n",
    "You might have noticed that data frames are rendered in nice HTML tables within Jupyter Notebooks. For small data frames, just showing all the data makes sense, but for larger datasets, like our `hit_albums` dataset, plotting 70+ rows can be annoying, and for datasets with hundreds or thousands of rows it can be prohibitive. By default, a data frame only prints a limited number of elements (notice the `...` of the output of `hit_albums` above – only the first and last few are printed. \n",
    "\n",
    "When working with data, e.g., when transforming or loading a dataset, it is important to see the raw data to, for example, check if a transformation was done correctly. Often, however, it's sufficient to see a part of the data, e.g., the first couple of rows and/or the last couple of rows. We can do this with the `head()` and `tail()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head shows the first 5 rows of a datset\n",
    "hit_albums.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can specify how much to show\n",
    "hit_albums.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tail shows the last five rows in a datasaet\n",
    "hit_albums.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, if we really want to see as many rows as possible, we can use this option: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we want to see all the rows\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "hit_albums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check out the dimensions of the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we learn that our dataset has 77 rows and 6 columns.\n",
    "\n",
    "We can also get more info about the dataset using the `info()` method, which is especially helpful to see the data types of the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for series, we can get a rough description of the numerical values of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't see any descriptions of the columns of non-numerical type. We can, however, get a summary by directly accessing a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums[\"Artist\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that Michael Jackson is the top artist in this list, with five albums. Are there other artists with multiple albums in the list? We can answer that question with the value_counts() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums[\"Artist\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at whether the numerical columns in our data frame are correlated using the [`corr()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.corr.html) method. By default, this calculates a Pearson correlation between the column, excluding NaN values. Not surprisingly, we see a rather strong correlation (0.81) between certified and claimed sales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do transpose a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing Data Frames\n",
    "\n",
    "A common task is to create subsets of a dataframe. Check out the official [user guide for more info on this](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html).  \n",
    "\n",
    "Column access/slicing works (also) by directly using brackets `[]` on the data frame. Row access/slicing works by using the `.loc[]` indexer (more details later).\n",
    "\n",
    "We can explicitly define the the **column(s)** we want by their lables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a single column\n",
    "hit_albums[\"Artist\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use an array of lables if we want multiple columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying multiple columns in an array\n",
    "hit_albums = hit_albums[[\"Artist\",\"Certified sales (millions)\", \"Claimed sales (millions)\"]]\n",
    "hit_albums.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing of columns requires the `iloc` operator\n",
    "\n",
    "This: \n",
    "```python\n",
    "hit_albums[\"Artist\":\"Genre\"]\n",
    "```\n",
    "Doesn't work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One break with the convention that rows have to be accessed via `loc` or `iloc` is simple numerical slicing. The documentation claims that's for convenience, since this is so common: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these access methods we can also update the order: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums[[\"Certified sales (millions)\", \"Claimed sales (millions)\", \"Artist\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set a column to be the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums_reindexed = hit_albums.set_index(\"Artist\")\n",
    "hit_albums_reindexed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slicing with `loc`\n",
    "\n",
    "So far, we've said that `loc` is for rows, and `[]` is for columns. However, we can retrieve **rows** and **columns** using the `loc` indexer. Generally, `loc` is preferred over direct access via brackets for production systems. \n",
    "\n",
    "The syntax is:\n",
    "\n",
    "```\n",
    "df.loc[rows, columnns]\n",
    "```\n",
    "Here rows, columns can be explicit labels, lists of labels, or slicing operators. \n",
    "\n",
    "Here's a simple example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums_reindexed.loc[\"Meat Loaf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And an example with multiple keys that returns a data frame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums_reindexed.loc[\"Michael Jackson\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second argument in the `loc` array can be used to access columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums_reindexed.loc[\"Michael Jackson\", \"Certified sales (millions)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example with a list of row labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums_reindexed.loc[[\"Michael Jackson\", \"Meat Loaf\"], \"Certified sales (millions)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use slice operations. Remember, that slicing by label (`loc`) includes the last value, by index (`iloc`) does not. \n",
    "\n",
    "\n",
    "Note that this doesn't work if we use labels that have duplicates as indexers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums_reindexed.loc[\"Green Day\":\"Supertramp\"]\n",
    "\n",
    "# this wouldn't work\n",
    "#hit_albums_reindexed.loc[\"Green Day\":\"Michael Jackson\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can also be combined with slicing columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums_reindexed.loc[\"Green Day\":\"Supertramp\", [\"Certified sales (millions)\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also slice columns with loc. Let's re-load the full dataset first:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_hit_albums = pd.read_csv(\"hit_albums.csv\")\n",
    "full_hit_albums.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a slice containing all rows and the columns from \"Artist\" to \"Released\". Note the `[:,` syntax: this indicates that we want all the rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_hit_albums.loc[:,\"Artist\":\"Released\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a slice for the first 11 rows (up to including index 10) and columns \"Artists\" to \"Released\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_hit_albums.loc[:10,\"Artist\":\"Released\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the same thing using `iloc`, i.e., index based slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_hit_albums.iloc[0:11, 0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting\n",
    "\n",
    "Of course, we can use broadcasting and filtering based on boolean masks just as we do for series.\n",
    "\n",
    "Here we boradcast an operation on a series and set it to a new column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_hit_albums[\"Certified sales\"] =  full_hit_albums[\"Certified sales (millions)\"] * 1000000\n",
    "full_hit_albums.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's filter out all of the albums that were released before 1990. We create a boolean series first: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = full_hit_albums[\"Released\"] > 1990\n",
    "mask.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then apply this to the data frame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_hit_albums.loc[mask].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, in short: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_hit_albums.loc[full_hit_albums[\"Released\"] > 1990].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling NaN Values\n",
    "\n",
    "As with a series, we can drop all rows that contain NaN values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums.dropna().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, that's pretty aggressive here – we're dropping more than half of our dataset. We could also just remove the claimed sales. \n",
    "\n",
    "An alternative would be to fill the missing values with our best guess: \n",
    "We can use the [`fillna()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html) on it.\n",
    "\n",
    "We could replace all NaN values with 0s: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums.fillna(0).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thought it's probably better to use the forward fill (`ffill`) method here. By default, `ffill` will use the value of the previous row to fill a NaN value: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums.fillna(method=\"ffill\").tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It probably makes more sense to use forward fill along the columns, so that the certified sales are filled into the claimed sales if necessary. We can do that by specifying `axis=1` so that `ffill` works on the columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the result here\n",
    "hit_albums = hit_albums.fillna(axis=1, method='ffill')\n",
    "hit_albums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping Data Frames\n",
    "\n",
    "Very often, we want to aggregate data. Given the hit-albums dataset, for example, we might want to ask how many albums each artist in that list has sold in total. We can do these aggregations using the [group-by method](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify a column to group by, for example, \"Artist\". We can look at the groups created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = hit_albums.groupby(\"Artist\")\n",
    "grouped.groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the keys map to a set of indices. For example, Michael Jackson's albums are found at indices [0, 10, 16, 65, 66].\n",
    "\n",
    "Once we have created these groups, we can specify what to do with it.\n",
    "\n",
    "Pandas has a couple of built in functions to make this easy. For example, we can just call `sum()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.sum().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that we've summed up the data for each column. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we sort them, and have a nice result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.sum().sort_values(\"Certified sales (millions)\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative is the `count` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_count = grouped.count().sort_values(\"Certified sales (millions)\", ascending=False).head(10)\n",
    "album_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should probably rename the column here to show that it's a count: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_count[\"Count\"] = album_count[\"Certified sales (millions)\"]\n",
    "album_count[\"Count\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A very generic solution is the `agg()` function, which we can pass a function to do things with the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we pass the sum function, which calcualtes the sum of a list, to the group\n",
    "grouped.agg(sum).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an aggregation with an in-line function definition where we still create the sum, but also multiply by a million. We use a [lambda expression](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions) to define the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.agg(lambda rows : sum([cell * 1000000 for cell in rows])).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda expressions are just a different way of defining a function in line, without assigning it a name. In Python, they only work for a single statement. \n",
    "\n",
    "Here is a simple lambda expression, which returns a function, which we assign to the variable add:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add = lambda a, b : a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions defined via a lambda expression don't have to have a name – that's why they're also called \"anonymous functions\". We can assign them to a variable though, as we did above. \n",
    "\n",
    "The parameters in a function are specified after the lambda keyword, the body of the function is specified after the colon:  \n",
    "\n",
    "```python\n",
    "lambda a, b : a+b\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's take this apart: \n",
    "\n",
    "```python\n",
    "[cell * 1000000 for cell in rows]\n",
    "```\n",
    "\n",
    "This part is a list comprehension, that takes an array `rows` and multiplies every element with 1,000,000. This changes the count of every album in the groups. \n",
    "\n",
    "The surrounding `sum` is a call to the sum function, so adds up the values in the just modified array. \n",
    "\n",
    "And finally, the lambda expression packs all of this in a function. \n",
    "\n",
    "Here is a different, longer way to write this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumUpAndMultiplyByMillion(rows):\n",
    "    multiplied_rows = [cell * 1000000 for cell in rows]\n",
    "    summed_value = sum(multiplied_rows)\n",
    "    return summed_value\n",
    "\n",
    "grouped.agg(sumUpAndMultiplyByMillion).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buit-in Plotting\n",
    "\n",
    "Dataframes have built-in plotting capabilities based on the [matplotlib](http://matplotlib.org/) library. We'll see more about plotting later – here we'll only use the built-in capabilities of pandas. \n",
    "\n",
    "First, we have to import the matplotlib library, and tell Jupyter to display the images directly here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "# This next line tells jupyter to render the images inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can simply call the plot attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also selected certain columns using labelled indexes and then plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums[\"Certified sales (millions)\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use bar-charts instead of line-charts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums[[\"Certified sales (millions)\", \"Claimed sales (millions)\"]].plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default is a line chart. This doesn't make much sense, since it's mixing index of the row with sales. We're better off plotting only the two different sales figures.\n",
    "\n",
    "A better way to compare certified and claimed sales is a scatterplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums.plot.scatter(x=\"Certified sales (millions)\", y=\"Claimed sales (millions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or a histogram: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_albums.plot.hist(bins=12, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We'll do more sophisticated plotting in the future! "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "nbpresent": {
   "slides": {
    "19a6495f-8346-4b23-a98a-c7115941e8f0": {
     "id": "19a6495f-8346-4b23-a98a-c7115941e8f0",
     "prev": null,
     "regions": {
      "d6523b36-7204-4001-8a6c-37c431b18d26": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "id": "d6523b36-7204-4001-8a6c-37c431b18d26"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
